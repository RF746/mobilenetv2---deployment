# -*- coding: utf-8 -*-
"""mobilenetv2_inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Md-8TVqHIgEOQMpcjkBazeLFdUv1F7N

# MobileNetV2: Load, Preprocess, and Run Inference
"""

import torch
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import requests
from io import BytesIO
import os
import numpy as np
import time

model = models.mobilenet_v2(pretrained=True)
model.eval()

url = "https://upload.wikimedia.org/wikipedia/commons/2/26/YellowLabradorLooking_new.jpg"
response = requests.get(url)
img = Image.open(BytesIO(response.content))
img.show()

preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
input_tensor = preprocess(img)
input_batch = input_tensor.unsqueeze(0)

with torch.no_grad():
    output = model(input_batch)
predicted_idx = torch.argmax(output[0]).item()

!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt -O imagenet_classes.txt
with open("imagenet_classes.txt") as f:
    labels = [line.strip() for line in f.readlines()]
print(f"Predicted label: {labels[predicted_idx]}")

"""# Phase 2: Convert MobileNetV2 to ONNX and Analyze"""

try:
    import onnx
except ImportError:
    !pip install onnx
    import onnx

dummy_input = torch.randn(1, 3, 224, 224)
onnx_filename = "mobilenetv2.onnx"
torch.onnx.export(model, dummy_input, onnx_filename,
                  input_names=["input"], output_names=["output"],
                  dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
                  opset_version=11)
print("Exported model to ONNX format.")

file_size_mb = os.path.getsize(onnx_filename) / 1e6
print(f"ONNX model size: {file_size_mb:.2f} MB")

!pip install onnxruntime
import onnxruntime as ort
ort_session = ort.InferenceSession(onnx_filename)

onnx_input = input_batch.numpy()
onnx_output = ort_session.run(None, {"input": onnx_input})
onnx_pred_idx = np.argmax(onnx_output[0])
print(f"ONNX Predicted label: {labels[onnx_pred_idx]}")

# Inference time comparison
start = time.perf_counter()
with torch.no_grad():
    _ = model(input_batch)
end = time.perf_counter()
print(f"PyTorch inference time: {end - start:.4f} sec")
start = time.perf_counter()
_ = ort_session.run(None, {"input": onnx_input})
end = time.perf_counter()
print(f"ONNX inference time: {end - start:.4f} sec")

"""# Phase 3: Quantize ONNX Model for Edge Deployment"""

from onnxruntime.quantization import quantize_dynamic, QuantType

quantized_model_path = "mobilenetv2_quantized.onnx"
quantize_dynamic(onnx_filename, quantized_model_path, weight_type=QuantType.QUInt8)

print("Quantization complete.")

quantized_size_mb = os.path.getsize(quantized_model_path) / 1e6
print(f"Quantized ONNX model size: {quantized_size_mb:.2f} MB")

# Load and run quantized model
quantized_session = ort.InferenceSession(quantized_model_path)

start = time.perf_counter()
_ = quantized_session.run(None, {"input": onnx_input})
end = time.perf_counter()

print(f"Quantized ONNX inference time: {end - start:.4f} sec")

print("âœ… All steps complete!")
print(f"Final Quantized Model Size: {quantized_size_mb:.2f} MB")
print(f"Final Inference Time (Quantized): {end - start:.4f} sec")
print(f"Predicted Label: {labels[onnx_pred_idx]}")