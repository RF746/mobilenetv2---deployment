
# ğŸ§  MobileNetV2 Edge Deployment Optimization â€“ PM & Engineering Hybrid Workflow

This project simulates a full-cycle AI software delivery initiative, executed with the mindset of a Technical Project Manager operating within a hardware-aware environment. The workflow demonstrates how a model is planned, scoped, optimized, and benchmarked for edge AI use â€” aligning with the performance and efficiency needs of AI-centric compute platforms.

---

## ğŸ“ Project Objective

**Deliver an optimized, portable AI model pipeline** that simulates deployment on edge/embedded hardware (like custom chips or RISC-V-based systems), balancing inference speed, file size, and runtime compatibility.

---

## ğŸ”§ Project Management Methodology

- **Approach:** Agile-inspired phase structure
- **Planning:** Phased backlog (PyTorch â†’ ONNX â†’ Quantized ONNX)
- **Execution:** Weekly milestones & sprint-style review
- **Tracking Tools:** Task board matrix (included below)
- **PM Mindset:** Scope â†’ Optimize â†’ Validate â†’ Report

---

## ğŸ› ï¸ Stack + Tooling

| Category | Tool |
|---------|------|
| Modeling & Inference | PyTorch, ONNX |
| Optimization | ONNX Runtime Quantization |
| Measurement & Analysis | Python, Matplotlib, `time`, file size profiling |
| Delivery Environment | Google Colab |
| Docs & Visuals | Markdown, Matplotlib, Google Workspace |

---

## ğŸš¦ Key Deliverables

- âœ… Model successfully loaded, converted, and tested across formats
- âœ… Quantized model reduced by ~72% with sub-100ms inference time
- âœ… ONNX Runtime used to validate quantized format under CPU-only constraints
- âœ… Runtime errors diagnosed and resolved (ConvInteger â†’ fallback quantization)
- âœ… Bar charts generated to compare size and speed across formats
- âœ… README + Visuals delivered as stakeholder-facing final report

---

## ğŸ” Technical Results

| Metric                     | Value             |
|---------------------------|-------------------|
| Original ONNX Model Size  | ~13 MB            |
| Quantized Model Size      | **3.69 MB**       |
| Inference Time (Quantized)| **0.0715 sec**    |
| Format Compatibility      | âœ… ONNX Runtime |


### ğŸ“Š Visual Insights

![Model Size](model_size_comparison.png)  
*Model size reduction across formats*

![Inference Time](inference_time_comparison.png)  
*Latency improvement for quantized ONNX*

---

## ğŸ—‚ï¸ Task Tracking Board (Sprint Simulation)

| Task | Description | Status | Priority |
|------|-------------|--------|----------|
| Load pretrained model | Load MobileNetV2 in PyTorch and verify inference | âœ… Done | High |
| Export model to ONNX | Convert model for cross-platform use | âœ… Done | High |
| Run ONNX inference | Validate ONNX output | âœ… Done | High |
| Quantize ONNX model | Optimize model for edge hardware | âœ… Done | High |
| Resolve ONNX Runtime errors | Adjust for unsupported Conv ops | âœ… Done | High |
| Benchmark & visualize | Generate metrics and performance charts | âœ… Done | Medium |
| Write documentation | Create README + visual summaries | âœ… Done | High |
| Simulate PM workflow | Structure execution like a product sprint | âœ… Done | Medium |

---

## ğŸ”‘ Key Skills Demonstrated

- ğŸ§  **AI Optimization & Quantization**
- ğŸ“‹ **Agile Sprint Planning**
- âš™ï¸ **Problem Solving (Runtime Debugging)**
- ğŸ§¾ **Stakeholder-Ready Documentation**
- ğŸ“ˆ **Benchmark Analysis & Reporting**
- ğŸ› ï¸ **Tool Integration (PyTorch, ONNX, Matplotlib, Colab)**

---

## ğŸ“ About the Author

**Ryan Faxigue**  
Software Engineer | AI Product Builder | Embedded Systems & PM Advocate

I build scalable AI solutions and manage technical initiatives from planning to deployment â€” always balancing user impact with real-world hardware performance needs.

ğŸ“« Connect:  
- [LinkedIn](https://www.linkedin.com/in/ryanfaxigue)  
- [GitHub](https://github.com/ryanfaxigue)

---

## ğŸ”­ Future Expansion

- TFLite Conversion for Mobile
- Raspberry Pi Inference Benchmarking
- Streamlit Dashboard for Visual QA
