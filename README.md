
# ğŸ” MobileNetV2 Inference & Quantization Pipeline

This project demonstrates how to load a pretrained MobileNetV2 model using PyTorch, convert it to ONNX, apply quantization for edge deployment, and run inference across all stages â€” with performance comparisons. It simulates a real-world AI deployment workflow, optimized for resource-constrained environments.

---

## ğŸ“Œ Overview

| Phase | Description |
|-------|-------------|
| âœ… Phase 1 | Load and run MobileNetV2 in PyTorch |
| âœ… Phase 2 | Convert PyTorch model to ONNX and test inference |
| âœ… Phase 3 | Apply dynamic quantization to reduce model size |
| âœ… Phase 4 | Run quantized model using ONNX Runtime and compare performance |

---

## ğŸ§  Why This Matters

This project replicates the type of work done in AI-focused hardware companies (e.g., Tenstorrent), where performance, portability, and compatibility are crucial. It demonstrates:

- Model optimization for edge devices
- ONNX export for hardware interoperability
- Quantization to reduce size and improve inference speed

---

## ğŸ› ï¸ Tech Stack

- **Python 3**
- **PyTorch**
- **Torchvision**
- **ONNX**
- **ONNX Runtime**
- **Google Colab (CPU)**

---

## ğŸ–¼ï¸ Sample Input

A sample image of a Labrador Retriever is passed through the model. The same image is tested across all phases to compare performance and label consistency.

---

## ğŸ“Š Results

| Metric                     | Value             |
|---------------------------|-------------------|
| Original ONNX Model Size  | ~13 MB            |
| Quantized ONNX Model Size | **3.69 MB**       |
| Inference Time (Quantized)| **0.0715 sec**    |
| Predicted Label           | **Labrador Retriever** |

---

## ğŸ“ Files

- `mobilenetv2_inference.ipynb` â€” Full notebook (PyTorch â†’ ONNX â†’ Quantization)
- `mobilenetv2.onnx` â€” Standard ONNX model
- `mobilenetv2_quantized.onnx` â€” Quantized ONNX model

---

## ğŸš€ How to Run

1. Open `mobilenetv2_inference.ipynb` in [Google Colab](https://colab.research.google.com)
2. Run all cells (Runtime > Run all)
3. Optional: Replace the test image URL with your own

---

## ğŸ™‹ About the Author

**Ryan Faxigue**  
Computer Science + Applied Math | AI/Cloud Engineering  
Passionate about AI deployment, embedded systems, and building scalable, optimized solutions.

---

## ğŸ“« Contact

Want to collaborate or connect?

- [LinkedIn](https://www.linkedin.com/in/ryan-n-faxigue-b21813229/)
- [GitHub](https://github.com/RF746)

---

## ğŸ Future Work

- Add TensorFlow Lite conversion
- Benchmark on Raspberry Pi or Jetson Nano
- Integrate with real-time camera input

---
